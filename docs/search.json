[
  {
    "objectID": "daily.html",
    "href": "daily.html",
    "title": "Daily",
    "section": "",
    "text": "Sakana\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nChoCho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nChoCho\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "temp/downloadthis.html",
    "href": "temp/downloadthis.html",
    "title": "Downloadthis Example",
    "section": "",
    "text": "Download \n Download the mtcars data \n Download the pdf"
  },
  {
    "objectID": "temp/lordicon.html",
    "href": "temp/lordicon.html",
    "title": "Lordicon Quarto Extension",
    "section": "",
    "text": "This extension allows you to use lordicon icons in your Quarto HTML documents.\n\nShortcodes\nThe {{< li >}} shortcode renders an icon (specified by its code) after downloading it the lordicon CDN. The {{< lif >}} shortcode renders an icon (specified by its filepath) from a local .json file. Both shortcodes support the same arguments for customization, described below.\n\n\n\n\n\n\n\n\nPseudocode\nExample Code\nRendered\n\n\n\n\n{{< li code >}}\n{{< li wlpxtupd >}}\n\n\n\n{{< lif file >}}\n{{< lif church.json >}}\n\n\n\n\n\n\nTriggers\ntrigger controls the icon’s animation type. When using the loop or loop-on-hover triggers, you can also set an optional delay (in ms) between loops.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{< li wxnxiano >}}\n\n\n\n{{< li wxnxiano trigger=click >}}\n\n\n\n{{< li wxnxiano trigger=hover >}}\n\n\n\n{{< li wxnxiano trigger=loop >}}\n\n\n\n{{< li wxnxiano trigger=loop delay=1000 >}}\n\n\n\n{{< li wxnxiano trigger=loop-on-hover >}}\n\n\n\n{{< li wxnxiano trigger=loop-on-hover delay=1000 >}}\n\n\n\n{{< li wxnxiano trigger=morph >}}\n\n\n\n{{< li wxnxiano trigger=boomerang >}}\n\n\n\n\n\n\nSpeed\nspeed controls how quickly the icon’s animation plays.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{< li lupuorrc trigger=loop speed=0.5 >}}\n\n\n\n{{< li lupuorrc trigger=loop speed=1.0 >}}\n\n\n\n{{< li lupuorrc trigger=loop speed=2.0 >}}\n\n\n\n\n\n\nColors\ncolors controls the icon’s coloring. Outline icons typically have just a primary and secondary color, but flat and lineal icons can have many more. Each color should be given in rank:color format (where ranks are primary, secondary, tertiary, etc.) and multiple colors should be separated by commas. Colors can be given in HTML color names or hexcodes.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{< li lupuorrc >}}\n\n\n\n{{< li lupuorrc colors=primary:gold >}}\n\n\n\n{{< li lupuorrc colors=primary:gray,secondary:orange >}}\n\n\n\n{{< li lupuorrc colors=primary:#4030e8,secondary:#ee66aa >}}\n\n\n\n\n\n\nStroke\nstroke controls how thick the lines in an icon are.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{< li lupuorrc stroke=50 >}}\n\n\n\n{{< li lupuorrc stroke=100 >}}\n\n\n\n{{< li lupuorrc stroke=150 >}}\n\n\n\n\n\n\nScale\nscale controls how large or zoomed in the icon is.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{< li lupuorrc scale=25 >}}\n\n\n\n{{< li lupuorrc scale=50 >}}\n\n\n\n{{< li lupuorrc scale=100 >}}\n\n\n\n\n\n\nAxis X\nx controls the horizontal position of the center of the icon.\n\n\n\nShortcode\nIcon\n\n\n\n\n{{< li lupuorrc x=25 >}}\n\n\n\n{{< li lupuorrc x=50 >}}\n\n\n\n{{< li lupuorrc x=100 >}}\n\n\n\n\n\n\nAxis Y\ny controls the vertical position of the center of the icon.\n\n\n\nShortcode\nIcon\n\n\n\n\n{{< li lupuorrc y=25 >}}\n\n\n\n{{< li lupuorrc y=50 >}}\n\n\n\n{{< li lupuorrc y=100 >}}"
  },
  {
    "objectID": "temp/social-embeds.html",
    "href": "temp/social-embeds.html",
    "title": "Social-embeds",
    "section": "",
    "text": "https://quarto-social-embeds.sellorm.com/examples/gists.html"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This document was published at image.jpg.\n\nEtiam eu suspendisse aliquam, suscipit augue, nulla auctor. Quis est litora ut aenean vulputate in aliquam nec massa lorem mus rhoncus. Bibendum hendrerit, a varius sed cubilia facilisis pellentesque. Nec commodo semper, malesuada malesuada in id sed. A pellentesque sed ac metus pellentesque. Sit sed eros vitae praesent duis facilisis. Non leo maecenas amet posuere semper sed semper elementum.\nPharetra eu est potenti eu. Semper tellus taciti sociosqu, sem est dui bibendum. Convallis vestibulum magnis velit, consequat sapien dis fusce, taciti luctus. Nec nec dapibus vel vulputate. Sed diam nulla, aptent posuere. At porttitor adipiscing sociis magna mus neque. Ullamcorper ac, et tempus, vel. Libero morbi ac amet rhoncus pellentesque lacus. Proin habitant venenatis netus luctus nunc sed dapibus. Fusce, posuere sapien et fusce fermentum nec sit sapien, imperdiet. In nullam curabitur at aliquet tristique pellentesque. Quis parturient lacinia duis eros sagittis maecenas imperdiet. Purus vivamus maecenas ut a. Ligula eros sapien ex ex nec sed eu sociosqu nascetur.\n\n\nWe know from the first fundamental theorem of calculus that for \\(x\\) in \\([a, b]\\):\n\\[\n\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).\n\\]\nThis is a post with executable code.11 ajdpiajdipa js\nSee Xie (2016).\n\nXie, Yihui. 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/bookdown.\n\n1 + 1\n\n[1] 2\n\n\nLorem ipsum dolor sit amet, qui minim labore adipisicing minim sint cillum sint consectetur cupidatat.\n\n1+1\n\n[1] 2"
  },
  {
    "objectID": "posts/observable-JS/ojs.html",
    "href": "posts/observable-JS/ojs.html",
    "title": "Function Plot",
    "section": "",
    "text": "See https://quarto.org/docs/interactive/ojs/.\nSee https://observablehq.com/@mauriciopoppe/function-plot.\nSee https://mauriciopoppe.github.io/function-plot/.\n\nfunctionPlot = require(\"function-plot@1.22.2/dist/function-plot\")\n\n\n\n\n\n\n\n{\n  let target = DOM.element('div')\n  functionPlot({\n    target: '#butterfly-curve',\n    yAxis: {domain: [-4.428571429, 4.428571429]},\n    xAxis: {domain: [-7, 7]},\n    data: [{\n      x: 'sin(t) * (exp(cos(t)) - 2 cos(4t) - sin(t/12)^5)',\n      y: 'cos(t) * (exp(cos(t)) - 2 cos(4t) - sin(t/12)^5)',\n      range: [-10 * Math.PI, 10 * Math.PI],\n      fnType: 'parametric',\n      graphType: 'polyline'\n    }],\n    target\n  });\n  return target\n}\n\n\n\n\n\n\n\n{\n  let target = DOM.element('div')\n  functionPlot({\n    title: 'y = x²',\n    data: [{\n      fn: 'x^2'\n    }],\n    target\n  });\n  return target\n}\n\n\n\n\n\n\n\ntargetA = {\n  let target = DOM.element('div')\n  return target\n}\ntargetB = {\n  let target = DOM.element('div')\n  return target\n}\njoinTargets = {\n  let a = functionPlot({\n    target: targetA,\n    xAxis: { domain: [-6, 6] },\n    yAxis: { domain: [-6, 6] },\n    annotations: [\n      {\n        y: 1,\n        text: 'max'\n      },\n      {\n        y: -1,\n        text: 'min'\n      }\n    ],\n    data: [\n      {\n        fn: '(x^3) / 3 - 2 * x * x + 3 * x + 2',\n        // fn: 'sin(x)',\n        graphType: 'polyline',\n        derivative: {\n          fn: 'x * x - 4 * x + 3',\n          // fn: 'cos(x)',\n          updateOnMouseMove: true\n        }\n      }\n    ]\n  })\n  let b = functionPlot({\n    target: targetB,\n    xAxis: { domain: [-4, 8] },\n    yAxis: { domain: [-4, 8] },\n    annotations: [\n      {\n        x: 1,\n        text: 'intercept'\n      },\n      {\n        x: 3,\n        text: 'intercept'\n      }\n    ],\n    data: [\n      {\n        fn: 'x * x - 4 * x + 3',\n        graphType: 'polyline'\n      }\n    ]\n  })\n  a.addLink(b)\n  b.addLink(a)\n  return \"targets joined!\"\n}"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/decorator.html",
    "href": "posts/decorator.html",
    "title": "Add to class 函數裝飾",
    "section": "",
    "text": "取自 https://d2l.ai/chapter_linear-regression/oo-design.html#utilities.\nSee https://www.runoob.com/w3cnote/python-func-decorators.html.\n重要準則:\n@f\ndef g():\n  ...\n等價於\ng = f(g)"
  },
  {
    "objectID": "posts/decorator.html#使用方式",
    "href": "posts/decorator.html#使用方式",
    "title": "Add to class 函數裝飾",
    "section": "使用方式",
    "text": "使用方式\n定義如下的 add_to_class.\n\ndef add_to_class(Class):\n  def wrapper(obj):\n    setattr(Class, obj.__name__, obj)\n  return wrapper\n\n假如我們有一個 class A, 且有個 method double.\n\nclass A:\n  def double(self,t):\n    return 2*t\n\n令 a 為 A 的 instance.\n\na = A()\n\n則我們可以呼叫 a.double(3).\n\na.double(3)\n\n6\n\n\n現在假如我們想修改 A 的 method double, 可用如下.\n\n@add_to_class(A)\ndef double(self,t):\n  return 7*t\n\n則可以發現: 不需重新令 a 是 A 的 instance, a 的 double 都已經變更.\n\na.double(3)\n\n21\n\n\n\n# 另一個 A 的 instance\naa = A()\naa.double(3)\n\n21"
  },
  {
    "objectID": "posts/decorator.html#解析-code",
    "href": "posts/decorator.html#解析-code",
    "title": "Add to class 函數裝飾",
    "section": "解析 code",
    "text": "解析 code\n回顧 add_to_class 為\ndef add_to_class(Class):\n  def wrapper(obj):\n    setattr(Class, obj.__name__, obj)\n  return wrapper\n且\n@add_to_class(A)\ndef double(self,t):\n  return 7*t\n等價於 double = add_to_class(A)(double).\n\n\n\n\n\n\nImportant\n\n\n\n這裡會令 double 為 add_to_class(A)(double). 所以要注意不會覆蓋到舊有的變數. 可以單獨執行 double 會發現不報錯 (不會 return 任何東西).\n\ndouble, type(double)\n\n(None, NoneType)\n\n\n\n\n所以有執行了 add_to_class(A)(double)1 這指令.1 這裡 add_to_class(A) 不 是 class, 而是 wrapper.\n所以有執行 setattr(Class, obj.__name__, obj), 其中 Class = A, obj = double.\n所以相當於執行了 setattr(A, double, 7*t)23.2 這裡的 obj.__name__ 相當於 obj 的名字. 在這就是 double.3 這裡 setattr(Class, obj.__name__, obj) 的第三分量 obj 相當於 double return 的東西, 所以就是 7*t.\n\n\nTODO:\n\nhttps://www.runoob.com/w3cnote/python-func-decorators.html"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Stochastic Gradient Descent\n\n\n2023-03-01\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nChoCho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\nChoCho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest css\n\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\nChoCho\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Archives",
    "section": "",
    "text": "Add to class 函數裝飾\n\n\n\n\n\n\n\ndecorator\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nChoCho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction Plot\n\n\n\n\n\n\n\nObservable JS\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nChoCho\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "daily/2023-02-14-sakana.html",
    "href": "daily/2023-02-14-sakana.html",
    "title": "Sakana",
    "section": "",
    "text": "https://github.com/itorr/sakana\n\n\n\n\n\n\nNibh nulla dictum sed ipsum, turpis. Mauris, dictum, etiam vestibulum id in felis aliquam. Suspendisse fermentum sodales, sit vehicula vulputate sed ligula integer et consectetur proin odio fusce praesent in. Nunc cum sed ridiculus habitasse, ad auctor convallis tincidunt nunc feugiat, platea. At, eget malesuada himenaeos nec, donec. A laoreet, erat lacus amet quisque pretium tincidunt. Non vel cum, tincidunt pharetra vitae et phasellus. Ante lectus donec eleifend, sed felis. Risus malesuada tellus aptent in turpis eu non erat ut molestie ut, curabitur euismod in felis. Luctus, sit mus a facilisi magna posuere.\nCurae maecenas ut id et nibh sed, nunc aliquam. In rhoncus vehicula, tempus habitasse vehicula ac sit quis efficitur orci ex. Tempor condimentum dictumst facilisi condimentum nascetur nibh fringilla mollis potenti in vitae. Id ac in, ante, blandit mauris. Et mauris aenean vel sodales. Commodo in sem in penatibus et mauris sem nulla. Facilisis hendrerit mi adipiscing maecenas facilisi a ac habitasse! Posuere aenean, mus et nam congue, suspendisse. Velit lacus, ullamcorper erat dictumst congue eget. Ridiculus a ac natoque lacinia turpis ipsum, nisi pulvinar etiam himenaeos potenti. Sit ultrices eget nostra diam at. Amet nunc iaculis ad ac sed elit lobortis.\nMorbi hendrerit velit nunc elementum ultricies. Sociis luctus nunc cum vivamus sed, ac ipsum mattis porttitor nec blandit scelerisque! Tempor sem lacinia sit aliquam fermentum in eu, taciti. Quis praesent, ante nostra orci ante senectus phasellus vitae turpis. Ligula, risus iaculis ut congue metus. Imperdiet vitae quisque donec posuere est vivamus neque ornare ac. Sed, donec aliquam ut, dictum accumsan! Ornare sed hendrerit taciti sit.\nDonec phasellus nulla nam, amet viverra. Inceptos nec fames ultricies a risus quam ridiculus lacinia lacus quisque. Nullam a posuere pharetra etiam risus tristique sit ac. Ac odio imperdiet pretium velit ac. At augue, egestas. Nunc sociis accumsan erat tristique proin senectus vestibulum quis faucibus lorem libero. Arcu interdum senectus nunc eget amet nec nulla vel. Tincidunt ipsum, ex nulla vestibulum torquent ac aenean arcu. Non justo, nisi elit eu vitae accumsan, parturient cubilia litora. Aliquet, egestas nibh id mauris, erat penatibus class aliquam. Pulvinar in. Aptent eu ad euismod aenean.\nNulla eu elit, sodales odio, ac nascetur. Blandit mus pulvinar dui nisi fusce. Neque cubilia aliquam cum in fermentum phasellus sed torquent. Metus iaculis mollis amet sed consequat himenaeos laoreet mauris ante. Ultricies vestibulum aliquam mattis pharetra et eu viverra mauris arcu. Euismod mattis venenatis. Nulla pellentesque. Et a nunc duis tincidunt adipiscing, sed."
  },
  {
    "objectID": "slides/slide-test2.html#modified",
    "href": "slides/slide-test2.html#modified",
    "title": "title",
    "section": "MODIFIED",
    "text": "MODIFIED"
  },
  {
    "objectID": "slides/slide-test2.html#yrsdf",
    "href": "slides/slide-test2.html#yrsdf",
    "title": "title",
    "section": "YRSdf",
    "text": "YRSdf\nLorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim labore culpa sint ad nisi Lorem pariatur mollit ex esse exercitation amet. Nisi animcupidatat excepteur officia. Reprehenderit nostrud nostrud ipsum Lorem est aliquip amet voluptate voluptate dolor minim nulla est proident. Nostrud officia pariatur ut officia. Sit irure elit esse ea nulla sunt ex occaecat reprehenderit commodo officia dolor Lorem duis laboris cupidatat officia voluptate. Culpa proident adipisicing id nulla nisi laboris ex in Lorem sunt duis officia eiusmod. Aliqua reprehenderit commodo ex non excepteur duis sunt velit enim. Voluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa duis."
  },
  {
    "objectID": "slides/slide-test1.html#modified",
    "href": "slides/slide-test1.html#modified",
    "title": "title",
    "section": "MODIFIED",
    "text": "MODIFIED"
  },
  {
    "objectID": "slides/slide-test1.html#section",
    "href": "slides/slide-test1.html#section",
    "title": "title",
    "section": "222",
    "text": "222\nLorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim labore culpa sint ad nisi Lorem pariatur mollit ex esse exercitation amet. Nisi animcupidatat excepteur officia. Reprehenderit nostrud nostrud ipsum Lorem est aliquip amet voluptate voluptate dolor minim nulla est proident. Nostrud officia pariatur ut officia. Sit irure elit esse ea nulla sunt ex occaecat reprehenderit commodo officia dolor Lorem duis laboris cupidatat officia voluptate. Culpa proident adipisicing id nulla nisi laboris ex in Lorem sunt duis officia eiusmod. Aliqua reprehenderit commodo ex non excepteur duis sunt velit enim. Voluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa duis."
  },
  {
    "objectID": "slides/slide-test1.html#title",
    "href": "slides/slide-test1.html#title",
    "title": "title",
    "section": "title",
    "text": "title"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog."
  },
  {
    "objectID": "slides/slide.html#modified",
    "href": "slides/slide.html#modified",
    "title": "title",
    "section": "MODIFIED",
    "text": "MODIFIED"
  },
  {
    "objectID": "slides/slide.html",
    "href": "slides/slide.html",
    "title": "title",
    "section": "",
    "text": "proof.\n\n\n\n\n\nLorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim labore culpa sint ad nisi Lorem pariatur mollit ex esse exercitation amet. Nisi animcupidatat excepteur officia. Reprehenderit nostrud nostrud ipsum Lorem est aliquip amet voluptate voluptate dolor minim nulla est proident. Nostrud officia pariatur ut officia. Sit irure elit esse ea nulla sunt ex occaecat reprehenderit commodo officia dolor Lorem duis laboris cupidatat officia voluptate. Culpa proident adipisicing id nulla nisi laboris ex in Lorem sunt duis officia eiusmod. Aliqua reprehenderit commodo ex non excepteur duis sunt velit enim. Voluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa duis.Lorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim labore culpa sint ad nisi Lorem pariatur mollit ex esse exercitation amet. Nisi animcupidatat excepteur officia. Reprehenderit nostrud nostrud ipsum Lorem est aliquip amet voluptate voluptate dolor minim nulla est proident. Nostrud officia pariatur ut officia. Sit irure elit esse ea nulla sunt ex occaecat reprehenderit commodo officia dolor Lorem duis laboris cupidatat officia voluptate. Culpa proident adipisicing id nulla nisi laboris ex in Lorem sunt duis officia eiusmod. Aliqua reprehenderit commodo ex non excepteur duis sunt velit enim. Voluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa duis."
  },
  {
    "objectID": "TODO.html",
    "href": "TODO.html",
    "title": "TODO",
    "section": "",
    "text": "https://cpok.tw/36122 租屋申請"
  },
  {
    "objectID": "slides/slide.html#proof.",
    "href": "slides/slide.html#proof.",
    "title": "title",
    "section": "proof.",
    "text": "proof.\nLorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim labore culpa sint ad nisi Lorem pariatur mollit ex esse exercitation amet. Nisi animcupidatat excepteur officia. Reprehenderit nostrud nostrud ipsum Lorem est aliquip amet voluptate voluptate dolor minim nulla est proident. Nostrud officia pariatur ut officia. Sit irure elit esse ea nulla sunt ex occaecat reprehenderit commodo officia dolor Lorem duis laboris cupidatat officia voluptate. Culpa proident adipisicing id nulla nisi laboris ex in Lorem sunt duis officia eiusmod. Aliqua reprehenderit commodo ex non excepteur duis sunt velit enim. Voluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa duis.Lorem ipsum dolor sit amet, officia excepteur ex fugiat reprehenderit enim labore culpa sint ad nisi Lorem pariatur mollit ex esse exercitation amet. Nisi animcupidatat excepteur officia. Reprehenderit nostrud nostrud ipsum Lorem est aliquip amet voluptate voluptate dolor minim nulla est proident. Nostrud officia pariatur ut officia. Sit irure elit esse ea nulla sunt ex occaecat reprehenderit commodo officia dolor Lorem duis laboris cupidatat officia voluptate. Culpa proident adipisicing id nulla nisi laboris ex in Lorem sunt duis officia eiusmod. Aliqua reprehenderit commodo ex non excepteur duis sunt velit enim. Voluptate laboris sint cupidatat ullamco ut ea consectetur et est culpa et culpa duis."
  },
  {
    "objectID": "slides/slide.html#ref",
    "href": "slides/slide.html#ref",
    "title": "title",
    "section": "REF",
    "text": "REF"
  },
  {
    "objectID": "slides/slide.html#sec-REF",
    "href": "slides/slide.html#sec-REF",
    "title": "title",
    "section": "REF",
    "text": "REF"
  },
  {
    "objectID": "slides/slide.html#resources",
    "href": "slides/slide.html#resources",
    "title": "title",
    "section": "Resources",
    "text": "Resources"
  },
  {
    "objectID": "slides/slide.html#more-resources",
    "href": "slides/slide.html#more-resources",
    "title": "title",
    "section": "More Resources",
    "text": "More Resources"
  },
  {
    "objectID": "slides/SGD.html#modified",
    "href": "slides/SGD.html#modified",
    "title": "Stochastic Gradient Descent",
    "section": "MODIFIED",
    "text": "MODIFIED"
  },
  {
    "objectID": "slides/SGD.html#lieanr-regression",
    "href": "slides/SGD.html#lieanr-regression",
    "title": "SGD",
    "section": "Lieanr Regression",
    "text": "Lieanr Regression\n\nWe have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R\\times \\mathbb R.\\)\nWe want to find some linear function \\(f:\\mathbb R\\longrightarrow \\mathbb R,\\) say \\(f(x) = w x+b,\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn math, our goal can be to minimize \\[\n\\begin{aligned}\n\\sum_{i=1}^{n} \\bigl( f(x^{(i)}) - y^{(i)} \\bigr)^2\n\\end{aligned},\n\\qquad(1)\\] where \\(w,b\\) are variables."
  },
  {
    "objectID": "slides/SGD.html#section",
    "href": "slides/SGD.html#section",
    "title": "SGD and MLP",
    "section": "",
    "text": "?@thm-a"
  },
  {
    "objectID": "slides/SGD.html#sgd",
    "href": "slides/SGD.html#sgd",
    "title": "Stochastic Gradient Descent",
    "section": "4.2 SGD",
    "text": "4.2 SGD\n\nRewrite ?@eq-sumf0218-1 to \\[\n\\begin{aligned}\n\\sum_{i=1}^n \\bigl( a^{(i)}\\theta - b^{(i)}  \\bigr)^2\n\\end{aligned}\n\\qquad(1)\\]\nWe have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R^{n_{\\text{in}}}\\times \\mathbb R^{n_{\\text{out}}}.\\)\nWe want to find some linear function \\(f:\\mathbb R^{n_{\\text{in}}} \\longrightarrow \\mathbb R^{n_{\\text{out}}}\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn mathematicas, we want \\[\n\\begin{aligned}\nTODO\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/SGD.html#mlp",
    "href": "slides/SGD.html#mlp",
    "title": "SGD and MLP",
    "section": "MLP",
    "text": "MLP"
  },
  {
    "objectID": "slides/SGD.html#lieanr-regression-in-mathbb-r1",
    "href": "slides/SGD.html#lieanr-regression-in-mathbb-r1",
    "title": "SGD",
    "section": "Lieanr Regression (in \\(\\mathbb R^1\\))",
    "text": "Lieanr Regression (in \\(\\mathbb R^1\\))\n\nWe have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R\\times \\mathbb R.\\)\nWe want to find some linear function \\(f:\\mathbb R\\longrightarrow \\mathbb R,\\) say \\(f(x) = w x+b,\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn math, our goal can be to minimize \\[\n\\begin{aligned}\n\\sum_{i=1}^{n} \\bigl( f(x^{(i)}) - y^{(i)} \\bigr)^2\n\\end{aligned},\n\\qquad(1)\\] where \\(w,b\\) are variables."
  },
  {
    "objectID": "slides/SGD.html#lieanr-regression-in-mathbb-r",
    "href": "slides/SGD.html#lieanr-regression-in-mathbb-r",
    "title": "SGD",
    "section": "Lieanr Regression (in \\(\\mathbb R\\))",
    "text": "Lieanr Regression (in \\(\\mathbb R\\))\n\nWe have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R\\times \\mathbb R.\\)\nWe want to find some linear function \\(f:\\mathbb R\\longrightarrow \\mathbb R,\\) say \\(f(x) = w x+b,\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn math, our goal can be to minimize \\[\n\\begin{aligned}\n\\sum_{i=1}^{n} d\\bigl( f(x^{(i)}) , y^{(i)} \\bigr)\n\\end{aligned},\n\\qquad(1)\\] where\n\n\\(w,b\\) are variables in \\(\\mathbb R\\) and\n\\(d:\\mathbb R^2 \\longrightarrow [0,\\infty)\\) is what we need to determine.\n\n\\(d?\\)\n\nBy Likelihood function."
  },
  {
    "objectID": "slides/SGD.html#section-1",
    "href": "slides/SGD.html#section-1",
    "title": "SGD and MLP",
    "section": "",
    "text": "We have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R^{n_{\\text{in}}}\\times \\mathbb R^{n_{\\text{out}}}.\\)\nWe want to find some linear function \\(f:\\mathbb R^{n_{\\text{in}}} \\longrightarrow \\mathbb R^{n_{\\text{out}}}\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn mathematicas, we want $$ \\begin{aligned}\n\n\\end{aligned} $$"
  },
  {
    "objectID": "slides/SGD.html#likelihood",
    "href": "slides/SGD.html#likelihood",
    "title": "SGD",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "slides/SGD.html#section-2",
    "href": "slides/SGD.html#section-2",
    "title": "SGD",
    "section": "",
    "text": "We have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R^{n_{\\text{in}}}\\times \\mathbb R^{n_{\\text{out}}}.\\)\nWe want to find some linear function \\(f:\\mathbb R^{n_{\\text{in}}} \\longrightarrow \\mathbb R^{n_{\\text{out}}}\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn mathematicas, we want $$ \\begin{aligned}\n\n\\end{aligned} $$"
  },
  {
    "objectID": "slides/SGD.html#section-3",
    "href": "slides/SGD.html#section-3",
    "title": "SGD",
    "section": "",
    "text": "We have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R^{n_{\\text{in}}}\\times \\mathbb R^{n_{\\text{out}}}.\\)\nWe want to find some linear function \\(f:\\mathbb R^{n_{\\text{in}}} \\longrightarrow \\mathbb R^{n_{\\text{out}}}\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn mathematicas, we want $$ \\begin{aligned}\n\n\\end{aligned} $$"
  },
  {
    "objectID": "slides/SGD.html#likelihood-function",
    "href": "slides/SGD.html#likelihood-function",
    "title": "Stochastic Gradient Descent",
    "section": "Likelihood function",
    "text": "Likelihood function\n\nblabla\nFor the linear regression, the best choice of \\(d\\) (in the sense of stastics) is \\(d(x,y)=(x-y)^2.\\)\nHow to find a local min?"
  },
  {
    "objectID": "slides/SGD.html#how-to-find-local-min",
    "href": "slides/SGD.html#how-to-find-local-min",
    "title": "Stochastic Gradient Descent",
    "section": "How to find local min?",
    "text": "How to find local min?\n\nFor the linear regression, the best choice of \\(d\\) (in the sense of stastics) is"
  },
  {
    "objectID": "slides/SGD.html#gd",
    "href": "slides/SGD.html#gd",
    "title": "Stochastic Gradient Descent",
    "section": "4.2 GD",
    "text": "4.2 GD"
  },
  {
    "objectID": "slides/SGD.html#linear-regression-in-mathbb-r",
    "href": "slides/SGD.html#linear-regression-in-mathbb-r",
    "title": "SGD and MLP",
    "section": "Linear Regression (in \\(\\mathbb R\\))",
    "text": "Linear Regression (in \\(\\mathbb R\\))\n\nWe have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R\\times \\mathbb R.\\)\nWe want to find some linear function \\(f:\\mathbb R\\longrightarrow \\mathbb R,\\) say \\(f(x) = w x+b,\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn math, our goal can be to minimize \\[\n\\begin{aligned}\n\\sum_{i=1}^{n} d\\bigl( f(x^{(i)}) , y^{(i)} \\bigr)\n\\end{aligned},\n\\qquad(1)\\] where\n\n\\(w,b\\) are variables in \\(\\mathbb R\\) and\n\\(d:\\mathbb R^2 \\longrightarrow [0,\\infty)\\) is what we need to determine.\n\n\\(d?\\)\n\nBy Likelihood function."
  },
  {
    "objectID": "slides/SGD.html#linear-regression",
    "href": "slides/SGD.html#linear-regression",
    "title": "SGD",
    "section": "Linear Regression",
    "text": "Linear Regression"
  },
  {
    "objectID": "slides/SGD.html#sgd-1",
    "href": "slides/SGD.html#sgd-1",
    "title": "SGD",
    "section": "SGD",
    "text": "SGD"
  },
  {
    "objectID": "slides/SGD.html#content",
    "href": "slides/SGD.html#content",
    "title": "SGD",
    "section": "Content",
    "text": "Content\nLinear Regression"
  },
  {
    "objectID": "slides/SGD_MLP.html#modified",
    "href": "slides/SGD_MLP.html#modified",
    "title": "SGD and MLP",
    "section": "MODIFIED",
    "text": "MODIFIED"
  },
  {
    "objectID": "slides/SGD_MLP.html#linear-regression-in-mathbb-r",
    "href": "slides/SGD_MLP.html#linear-regression-in-mathbb-r",
    "title": "SGD and MLP",
    "section": "Linear Regression (in \\(\\mathbb R\\))",
    "text": "Linear Regression (in \\(\\mathbb R\\))\n\nWe have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R\\times \\mathbb R.\\)\nWe want to find some linear function \\(f:\\mathbb R\\longrightarrow \\mathbb R,\\) say \\(f(x) = w x+b,\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn math, our goal can be to minimize \\[\n\\begin{aligned}\n\\sum_{i=1}^{n} d\\bigl( f(x^{(i)}) , y^{(i)} \\bigr)\n\\end{aligned},\n\\qquad(1)\\] where\n\n\\(w,b\\) are variables in \\(\\mathbb R\\) and\n\\(d:\\mathbb R^2 \\longrightarrow [0,\\infty)\\) is what we need to determine.\n\n\\(d?\\)\n\nBy Likelihood function."
  },
  {
    "objectID": "slides/SGD_MLP.html#likelihood-function",
    "href": "slides/SGD_MLP.html#likelihood-function",
    "title": "SGD and MLP",
    "section": "Likelihood function",
    "text": "Likelihood function"
  },
  {
    "objectID": "slides/SGD_MLP.html#how-to-find-local-min",
    "href": "slides/SGD_MLP.html#how-to-find-local-min",
    "title": "SGD and MLP",
    "section": "How to find local min?",
    "text": "How to find local min?"
  },
  {
    "objectID": "slides/SGD_MLP.html#gd",
    "href": "slides/SGD_MLP.html#gd",
    "title": "SGD and MLP",
    "section": "GD",
    "text": "GD\n\nRewrite Equation 1 to \\[\n\\begin{aligned}\n\\sum_{i=1}^n \\bigl( a^{(i)}\\theta - b^{(i)}  \\bigr)^2\n\\end{aligned}\n\\qquad(2)\\]\nhttps://baptiste-monpezat.github.io/blog/stochastic-gradient-descent-for-machine-learning-clearly-explained"
  },
  {
    "objectID": "slides/SGD_MLP.html#section",
    "href": "slides/SGD_MLP.html#section",
    "title": "SGD and MLP",
    "section": "",
    "text": "?@thm-a"
  },
  {
    "objectID": "slides/SGD_MLP.html#section-1",
    "href": "slides/SGD_MLP.html#section-1",
    "title": "SGD and MLP",
    "section": "",
    "text": "We have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R^{n_{\\text{in}}}\\times \\mathbb R^{n_{\\text{out}}}.\\)\nWe want to find some linear function \\(f:\\mathbb R^{n_{\\text{in}}} \\longrightarrow \\mathbb R^{n_{\\text{out}}}\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn mathematicas, we want $$ \\begin{aligned}\n\n\\end{aligned} $$"
  },
  {
    "objectID": "slides/SGD_MLP.html#sgd",
    "href": "slides/SGD_MLP.html#sgd",
    "title": "SGD and MLP",
    "section": "SGD",
    "text": "SGD"
  },
  {
    "objectID": "slides/SGD_MLP.html#mlp",
    "href": "slides/SGD_MLP.html#mlp",
    "title": "SGD and MLP",
    "section": "MLP",
    "text": "MLP"
  },
  {
    "objectID": "slides/SGD-MLP.html#modified",
    "href": "slides/SGD-MLP.html#modified",
    "title": "SGD and MLP",
    "section": "MODIFIED",
    "text": "MODIFIED"
  },
  {
    "objectID": "slides/SGD-MLP.html#linear-regression-in-mathbb-r",
    "href": "slides/SGD-MLP.html#linear-regression-in-mathbb-r",
    "title": "SGD and MLP",
    "section": "Linear Regression (in \\(\\mathbb R\\))",
    "text": "Linear Regression (in \\(\\mathbb R\\))\n\nWe have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R\\times \\mathbb R.\\)\nWe want to find some linear function \\(f:\\mathbb R\\longrightarrow \\mathbb R,\\) say \\(f(x) = w x+b,\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn math, our goal can be to minimize \\[\n\\begin{aligned}\n\\sum_{i=1}^{n} d\\bigl( f(x^{(i)}) , y^{(i)} \\bigr)\n\\end{aligned},\n\\qquad(1)\\] where\n\n\\(w,b\\) are variables in \\(\\mathbb R\\) and\n\\(d:\\mathbb R^2 \\longrightarrow [0,\\infty)\\) is what we need to determine.\n\n\\(d?\\)\n\nBy Likelihood function."
  },
  {
    "objectID": "slides/SGD-MLP.html#likelihood-function",
    "href": "slides/SGD-MLP.html#likelihood-function",
    "title": "SGD and MLP",
    "section": "Likelihood function",
    "text": "Likelihood function"
  },
  {
    "objectID": "slides/SGD-MLP.html#how-to-find-local-min",
    "href": "slides/SGD-MLP.html#how-to-find-local-min",
    "title": "SGD and MLP",
    "section": "How to find local min?",
    "text": "How to find local min?"
  },
  {
    "objectID": "slides/SGD-MLP.html#gd",
    "href": "slides/SGD-MLP.html#gd",
    "title": "SGD and MLP",
    "section": "GD",
    "text": "GD\n\nhttps://baptiste-monpezat.github.io/blog/stochastic-gradient-descent-for-machine-learning-clearly-explained"
  },
  {
    "objectID": "slides/SGD-MLP.html#section",
    "href": "slides/SGD-MLP.html#section",
    "title": "SGD and MLP",
    "section": "",
    "text": "We have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R^{n_{\\text{in}}}\\times \\mathbb R^{n_{\\text{out}}}.\\)\nWe want to find some linear function \\(f:\\mathbb R^{n_{\\text{in}}} \\longrightarrow \\mathbb R^{n_{\\text{out}}}\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn mathematicas, we want \\[\n\\begin{aligned}\nTODO\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/SGD-MLP.html#section-1",
    "href": "slides/SGD-MLP.html#section-1",
    "title": "SGD and MLP",
    "section": "",
    "text": "We have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R^{n_{\\text{in}}}\\times \\mathbb R^{n_{\\text{out}}}.\\)\nWe want to find some linear function \\(f:\\mathbb R^{n_{\\text{in}}} \\longrightarrow \\mathbb R^{n_{\\text{out}}}\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn mathematicas, we want $$ \\begin{aligned}\n\n\\end{aligned} $$"
  },
  {
    "objectID": "slides/SGD-MLP.html#sgd",
    "href": "slides/SGD-MLP.html#sgd",
    "title": "SGD and MLP",
    "section": "SGD",
    "text": "SGD"
  },
  {
    "objectID": "slides/SGD-MLP.html#mlp",
    "href": "slides/SGD-MLP.html#mlp",
    "title": "SGD and MLP",
    "section": "MLP",
    "text": "MLP"
  },
  {
    "objectID": "slides/SGD-MLP.html#sec-LR",
    "href": "slides/SGD-MLP.html#sec-LR",
    "title": "SGD and MLP",
    "section": "Linear Regression (in \\(\\mathbb R\\))",
    "text": "Linear Regression (in \\(\\mathbb R\\))\n\nWe have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R\\times \\mathbb R.\\)\nWe want to find some linear function \\(f:\\mathbb R\\longrightarrow \\mathbb R,\\) say \\(f(x) = w x+b,\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn math, our goal can be to minimize \\[\n\\begin{aligned}\n\\sum_{i=1}^{n} d\\bigl( f(x^{(i)}) , y^{(i)} \\bigr)\n\\end{aligned},\n\\qquad(1)\\] where\n\n\\(w,b\\) are variables in \\(\\mathbb R\\) and\n\\(d:\\mathbb R^2 \\longrightarrow [0,\\infty)\\) is what we need to determine.\n\n\\(d?\\)\n\nBy Likelihood function."
  },
  {
    "objectID": "slides/SGD-MLP.html#sec-LRinR",
    "href": "slides/SGD-MLP.html#sec-LRinR",
    "title": "SGD and MLP",
    "section": "Linear Regression (in \\(\\mathbb R\\))",
    "text": "Linear Regression (in \\(\\mathbb R\\))\n\nWe have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R\\times \\mathbb R.\\)\nWe want to find some linear function \\(f:\\mathbb R\\longrightarrow \\mathbb R,\\) say \\(f(x) = w x+b,\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn math, our goal can be to minimize \\[\n\\begin{aligned}\n\\sum_{i=1}^{n} d\\bigl( f(x^{(i)}) , y^{(i)} \\bigr)\n\\end{aligned},\n\\qquad(1)\\] where\n\n\\(w,b\\) are variables in \\(\\mathbb R\\) and\n\\(d:\\mathbb R^2 \\longrightarrow [0,\\infty)\\) is what we need to determine.\n\nWhat is the best choice for \\(d\\) (in some sense)?\n\nWe may use the Likelihood function."
  },
  {
    "objectID": "slides/SGD-MLP.html#sgd-1",
    "href": "slides/SGD-MLP.html#sgd-1",
    "title": "SGD and MLP",
    "section": "SGD",
    "text": "SGD\n\nRewrite Equation 1 to \\[\n\\begin{aligned}\n\\sum_{i=1}^n \\bigl( a^{(i)}\\theta - b^{(i)}  \\bigr)^2\n\\end{aligned}\n\\qquad(2)\\]\nWe have data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R^{n_{\\text{in}}}\\times \\mathbb R^{n_{\\text{out}}}.\\)\nWe want to find some linear function \\(f:\\mathbb R^{n_{\\text{in}}} \\longrightarrow \\mathbb R^{n_{\\text{out}}}\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn mathematicas, we want \\[\n\\begin{aligned}\nTODO\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/SGD-MLP.html#sgd-2",
    "href": "slides/SGD-MLP.html#sgd-2",
    "title": "SGD and MLP",
    "section": "SGD",
    "text": "SGD"
  },
  {
    "objectID": "slides/SGD-MLP.html#other-model",
    "href": "slides/SGD-MLP.html#other-model",
    "title": "SGD and MLP",
    "section": "Other Model",
    "text": "Other Model\n\nStone–Weierstrass Theorem."
  },
  {
    "objectID": "slides/SGD-MLP.html#idea",
    "href": "slides/SGD-MLP.html#idea",
    "title": "SGD and MLP",
    "section": "Idea",
    "text": "Idea\n\nPick some r.v. \\(X\\) s.t. \\(\\mathbb EX=\\nabla L(\\theta).\\)"
  },
  {
    "objectID": "slides/SGD.html#sec-LRegression-in-R",
    "href": "slides/SGD.html#sec-LRegression-in-R",
    "title": "Stochastic Gradient Descent",
    "section": "3.1 Linear Regression (in \\(\\mathbb R\\))",
    "text": "3.1 Linear Regression (in \\(\\mathbb R\\))\n\nSuppose that we have the data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R\\times \\mathbb R.\\)\n\nWe want to find some linear function \\(f:\\mathbb R\\longrightarrow \\mathbb R,\\) say \\(f(x) = w x+b,\\) s.t. \\[\n\\begin{aligned}\nf(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn math, our goal can be \\[\n\\begin{aligned}\n\\operatorname{argmin}_{w,b\\in \\mathbb R}\\sum_{i=1}^{n} \\ell \\bigl( y^{(i)} , f(x^{(i)}) \\bigr),\n\\end{aligned}\n\\] where \\(\\ell:\\mathbb R^2 \\longrightarrow [0,\\infty)\\) is what we need to determine.\nWhat is the best choice for \\(d\\) (in some sense)?\n\nWe may use the MLE (maximum likelihood function)."
  },
  {
    "objectID": "slides/SGD.html#idea",
    "href": "slides/SGD.html#idea",
    "title": "Stochastic Gradient Descent",
    "section": "4.4 Idea",
    "text": "4.4 Idea\n\nPick some r.v. \\(X\\) s.t. \\(\\mathbf E[X]=\\nabla L(\\theta).\\)"
  },
  {
    "objectID": "slides/SGD.html#other-model",
    "href": "slides/SGD.html#other-model",
    "title": "Stochastic Gradient Descent",
    "section": "5.1 Other Model",
    "text": "5.1 Other Model\n\nStone–Weierstrass Theorem.\n\n\n\n\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\nBottou, Léon. 2010. “Large-Scale Machine Learning with Stochastic Gradient Descent.” In Proceedings of COMPSTAT’2010, 177–86. Springer.\n\n\nLiu, Dong C, and Jorge Nocedal. 1989. “On the Limited Memory BFGS Method for Large Scale Optimization.” Mathematical Programming 45 (1): 503–28."
  },
  {
    "objectID": "slides/SGD.html#how-to-find-a-local-min",
    "href": "slides/SGD.html#how-to-find-a-local-min",
    "title": "Stochastic Gradient Descent",
    "section": "3.7 How to find a local min?",
    "text": "3.7 How to find a local min?\n\nWe want \\[\n\\begin{aligned}\n\\operatorname{argmin}_{w,b\\in \\mathbb R}\\sum_{i=1}^{n} \\bigl( f(x^{(i)}) - y^{(i)} \\bigr)^2.\n\\end{aligned}\n\\]\n\n\nObserving from the above graph, 從斜率的負方向走. This is GD."
  },
  {
    "objectID": "slides/SGD.html#mle-maximum-likelihood-function",
    "href": "slides/SGD.html#mle-maximum-likelihood-function",
    "title": "Stochastic Gradient Descent",
    "section": "MLE (maximum likelihood function)",
    "text": "MLE (maximum likelihood function)\nEX 2. 正常骰子 跟 12 面骰\nTODO"
  },
  {
    "objectID": "slides/SGD.html#asdasd",
    "href": "slides/SGD.html#asdasd",
    "title": "Stochastic Gradient Descent",
    "section": "1.1 asdasd",
    "text": "1.1 asdasd\n1.1.1 ASDASDAsimply connected domain\n1.1.2 asdajs pidj"
  },
  {
    "objectID": "slides/css-test.html#modified",
    "href": "slides/css-test.html#modified",
    "title": "Test css",
    "section": "MODIFIED",
    "text": "MODIFIED\n\n\nHoc vero non videre, maximoargumento esse voluptatem illam, qua sublata neget se intellegereomnino quid sit bonum - eam autem ita persequitur: quae palatopercipiatur, quae auribus; cetera addit, quae si appelles, honospraefandus sit - hoc igitur, quod solum bonum severus et gravisphilosophus novit, idem non videt ne expetendum quidem esse, quodeam voluptatem hoc eodem auctore non desideremus, cum dolorecareamus. quam haec sunt contraria! hic sidefinire, si dividere didicisset, si loquendi vim, si deniqueconsuetudinem verborum teneret, numquam in tantas salebrasincidisset. nunc vides, quid faciat. quam nemo umquam voluptatemappellavit, appellat; quae duo sunt, unum facit. hanc in motuvoluptatem - sic enim has suaves et quasi dulces voluptatesappellat - interdum ita extenuat, ut M’. Curium putes loqui, interdum ita laudat, ut quid praeterea sit bonum neget se posse nesuspicari quidem. quae iam oratio non a philosopho aliquo, sed acensore opprimenda est. non est enim vitium in oratione solum, sedetiam in moribus. luxuriam non reprehendit, modo sit vacua infinitacupiditate et timore. hoc loco discipulos quaerer!"
  },
  {
    "objectID": "temp/RNN.html",
    "href": "temp/RNN.html",
    "title": "RNN",
    "section": "",
    "text": "https://www.joet.org/m/journal/view.php?number=3090\n\n\n\nhttps://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45"
  },
  {
    "objectID": "temp/2023-02-22.html",
    "href": "temp/2023-02-22.html",
    "title": "Reading",
    "section": "",
    "text": "7.5 Batch normalize 可加深網路收斂速度\n\ntransform\ntransoorm"
  },
  {
    "objectID": "TODO.html#rnn",
    "href": "TODO.html#rnn",
    "title": "TODO",
    "section": "RNN",
    "text": "RNN\n\n菜EE機器人"
  },
  {
    "objectID": "slides/SGD.html#mle-maximum-likelihood-function-1",
    "href": "slides/SGD.html#mle-maximum-likelihood-function-1",
    "title": "Stochastic Gradient Descent",
    "section": "3.3 MLE (maximum likelihood function)",
    "text": "3.3 MLE (maximum likelihood function)\n\nWe regard \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) as realizations of random variables \\(X,Y,\\) and we assume that \\[\n\\begin{aligned}\nY=wX+b+\\mathtt{noise},\n\\end{aligned}\n\\] where\n\n\\(w,b\\) are two parameters in \\(\\mathbb R,\\) and\n\\(\\mathtt{noise}\\sim N(0,\\sigma^2)\\) for some fixed \\(\\sigma>0.\\)\n\nIn math, we give the joint density of \\((X,Y)\\) by the following:\n\nAssume that \\(X\\sim \\operatorname{unif}\\bigl(\\bigl[\\min_i x^{(i)}, \\max_i x^{(i)}\\bigr]\\bigr).\\) That is, the density of \\(X\\) is \\[\n\\begin{aligned}\n  f_X(x) = \\frac{1}{\\max_i x^{(i)}- \\min_i x^{(i)}} x.\n\\end{aligned}\n\\]\nConditioning on \\(X=x,\\) we give the density of \\(\\mathbb E[Y\\vert X]\\) by \\[\n\\begin{aligned}\n  Y-(wx+b) \\sim N(0,\\sigma^2).\n\\end{aligned}\n\\] That is, the density of \\(Y,\\) conditioning on \\(X=x,\\) is \\[\n\\begin{aligned}\n  f_{Y\\vert X} (y\\vert x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\Bigl\\lbrace - \\frac{1}{2\\sigma^2} \\bigl( y-(wx+b) \\bigr)^2 \\Bigr\\rbrace.\n\\end{aligned}\n\\]\nThe joint density of \\((X,Y)\\) is \\[\n\\begin{aligned}\n  f_{X,Y}(x,y) = f_{Y\\vert X}(y\\vert x) \\cdot f_X(x).\n\\end{aligned}\n\\]\n\nLet \\((X_i,Y_i)\\) be i.i.d. with \\((X_i,Y_i)\\stackrel{d}{=} (X,Y)\\) and \\(Z_i = \\mathbb E[Y_i\\vert X_i].\\)\nThe likelihood function of \\(\\bigl\\lbrace Z_i:i=1\\sim n \\bigr\\rbrace,\\) for the realizations \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) is \\[\n\\begin{aligned}\n  \\mathtt{lik}(w,b)\\Big\\vert_{(x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})}\n  = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\Bigl\\lbrace - \\frac{1}{2\\sigma^2} \\bigl( y^{(i)}-(wx^{(i)}+b) \\bigr)^2 \\Bigr\\rbrace.\n\\end{aligned}\n\\]\nTo maximize \\(\\mathtt{lik}(w,b)\\Big\\vert_{(x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})}\\) \\(\\iff\\) To minimize \\(\\sum_{i=1}^n \\bigl( y^{(i)}-(wx^{(i)}+b) \\bigr)^2.\\)\nFor the linear regression, the best choice of \\(d\\) (in the sense of stastics) is \\(d(x,y)=(x-y)^2.\\)\nHow to find a local min?"
  },
  {
    "objectID": "slides/SGD.html#dl-and-statistic",
    "href": "slides/SGD.html#dl-and-statistic",
    "title": "Stochastic Gradient Descent",
    "section": "3.4 DL and statistic",
    "text": "3.4 DL and statistic"
  },
  {
    "objectID": "slides/SGD.html#sec-SGD",
    "href": "slides/SGD.html#sec-SGD",
    "title": "Stochastic Gradient Descent",
    "section": "4.3 SGD",
    "text": "4.3 SGD\n\nFor simplicity, we assume that \\(b\\) is a constant, not a parameter.\nRewriting \\[\n\\begin{aligned}\n  \\sum_{i=1}^n \\bigl( y^{(i)} - (w x^{(i)} + b) \\bigr)^2\n\\end{aligned}\n\\] to \\[\n\\begin{aligned}\n  \\sum_{i=1}^n \\bigl( a^{(i)}\\theta - b^{(i)}  \\bigr)^2.\n\\end{aligned}\n\\qquad(1)\\]\n\n4.3.1 Idea\n\nPick some r.v. \\(X\\) s.t. \\(\\mathbf E[X]=\\nabla L(\\theta).\\)"
  },
  {
    "objectID": "slides/SGD.html#sec-SGD2",
    "href": "slides/SGD.html#sec-SGD2",
    "title": "Stochastic Gradient Descent",
    "section": "4.2 SGD",
    "text": "4.2 SGD\n\nRewrite \\[\n\\begin{aligned}\n\\sum_{i=1}^n \\bigl( a^{(i)}\\theta - b^{(i)}  \\bigr)^2\n\\end{aligned}\n\\qquad(1)\\]"
  },
  {
    "objectID": "slides/SGD.html#sec-MLE",
    "href": "slides/SGD.html#sec-MLE",
    "title": "Stochastic Gradient Descent",
    "section": "MLE (maximum likelihood function)",
    "text": "MLE (maximum likelihood function)\n\n\nIf \\(X_1,\\cdots,X_n\\) are i.i.d. from \\(f(\\cdot \\vert \\theta_{\\text{real}}),\\) and let \\[\n\\begin{aligned}\n  \\text{lik}(\\theta) = \\prod_{i=1}^n f(X_i\\vert \\theta).\n\\end{aligned}\n\\]\nMLE of \\((X_1,\\cdots,X_n)\\) is \\[\n\\begin{aligned}\n  \\widehat{\\theta} = \\operatorname{argmin}_{\\theta\\in \\Theta} \\text{lik}(\\theta)\n\\end{aligned}\n\\]\nThen under some smoothness conditions of \\(f,\\) \\[\n\\begin{aligned}\n  \\sqrt{n} \\big( \\widehat{\\theta} -\\theta_{\\text{real}} \\big)\n\\end{aligned}\n\\] tends to a normal distribution."
  },
  {
    "objectID": "slides/SGD.html#temp",
    "href": "slides/SGD.html#temp",
    "title": "Stochastic Gradient Descent",
    "section": "4.1 TEMP",
    "text": "4.1 TEMP\n\nhttps://baptiste-monpezat.github.io/blog/stochastic-gradient-descent-for-machine-learning-clearly-explained\nhttps://kevinbinz.com/2019/05/26/intro-gradient-descent/\nWhy it works\nhttps://www.kaggle.com/code/ryanholbrook/stochastic-gradient-descent"
  },
  {
    "objectID": "slides/SGD.html#sec-minibatchSGD",
    "href": "slides/SGD.html#sec-minibatchSGD",
    "title": "Stochastic Gradient Descent",
    "section": "Minibatch Stochastic Gradient Descent",
    "text": "Minibatch Stochastic Gradient Descent\n\nGiven any initial value \\(\\theta\\in \\mathbb \\Theta\\) and any \\(\\eta\\in (0,\\infty)\\) small enough.\nGiven a number \\(n_B,\\) called the batch size.\nSplit \\(\\lbrace 1,2,\\cdots,n \\rbrace\\) into \\[\n\\begin{aligned}\n  \\mathcal{B}_1 = &\\lbrace 1,2,\\cdots, n_B \\rbrace,  \\cr\n  \\mathcal{B}_2 = &\\lbrace n_B+ 1, n_B+ 2,\\cdots, 2 n_B \\rbrace,  \\cr\n  &\\vdots \\cr\n  &  \\lbrace \\cdots, n \\rbrace.\n\\end{aligned}\n\\]\nLet \\(P\\) be a permutation of \\(\\lbrace 1,2,\\cdots,n \\rbrace.\\)\n\nFor \\(k=1,2,\\cdots,\\) we update \\(\\theta\\) by \\[\n\\begin{aligned}\n  \\theta \\leftarrow \\theta -  \\eta \\cdot \\frac{1}{n_B} \\sum_{j\\in \\mathcal{B}_k} \\nabla\\ell_{P(j)}(\\theta) .\n\\end{aligned}\n\\] We call an epoch if we run over all \\(k.\\)"
  },
  {
    "objectID": "slides/SGD.html#deep-learning-dl-and-statistic",
    "href": "slides/SGD.html#deep-learning-dl-and-statistic",
    "title": "Stochastic Gradient Descent",
    "section": "Deep learning (DL) and statistic",
    "text": "Deep learning (DL) and statistic\n\n\n\n\n\n\n\n\n\nDeep learning\nStatistic\n\n\n\n\n\\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\)\nData\nSample\n\n\n\nBuild a model \\(\\texttt{Net}_{\\theta}\\) (denote the estimators \\(\\mathtt{Net}_{\\theta}(x)\\) by \\(\\widehat{y}\\))\nRegard \\(x^{(i)},y^{(i)}\\) as the realizations of random elements \\(X,Y\\)\n\n\n\\(\\theta\\)\nparameters of model\nparameters of density or mass\n\n\noptimize parameters\nMin. \\(\\sum_{i=1}^n\\ell(y^{(i)},\\widehat{y}^{(i)}),\\) where \\(\\widehat{y}^{(i)}=\\mathtt{Net}_{\\theta}(x^{(i)})\\)\nMax. the likelihood\n\n\n\n\n\n\\(\\ell\\) is called the loss function.\nHow to minimize \\(\\sum_{i=1}^n\\ell(y^{(i)},\\widehat{y}^{(i)})?\\)\n\n \n\n\n\n\n這裡的變數是 \\(\\theta.\\) 注意有 data space and parameter space.\nObserving from the above graph, 從斜率的負方向走. This is Gradient Descent.\n之後會解釋為什麼叫 \\(\\mathtt{Net}.\\)"
  },
  {
    "objectID": "slides/SGD.html#gd-gradient-descent",
    "href": "slides/SGD.html#gd-gradient-descent",
    "title": "Stochastic Gradient Descent",
    "section": "GD (Gradient Descent)",
    "text": "GD (Gradient Descent)\n\n\n\nOur goal is to find a local min. of \\[\n\\begin{aligned}\n  L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n\\ell(y^{(i)},\\widehat{y}^{(i)}).\n\\end{aligned}\n\\]\nGiven any initial value \\(\\theta_0\\in \\mathbb \\Theta.\\)\nGiven any \\(\\eta\\in (0,\\infty)\\) small enough.\n\n\\(\\eta\\): learning rate.\n\nMomentum, Adam, \\(\\cdots.\\)\n\n\nWe set \\(\\lbrace \\theta_i \\rbrace_{i\\in \\mathbb N}\\) by \\[\n\\begin{aligned}\n\\theta_{i} \\leftarrow \\theta_{i-1} - \\eta \\cdot \\nabla L(\\theta_{i-1}),\\quad i\\in \\mathbb N.\n\\end{aligned}\n\\]\nThen \\(\\theta_i,i\\in \\mathbb N\\) may converges to a local min. of \\(L.\\)\n\n\n\n\n\n\n\n\n\nIn practice, this can be extremely slow: we must pass over the entire dataset before making a single update, even if the update steps might be very powerful (Liu and Nocedal 1989).\n\nSolution: Stochastic Gradient Descent (SGD).\n\n\n\n\n取 \\(1/n\\) 是之後有個方便地方."
  },
  {
    "objectID": "slides/SGD.html#why",
    "href": "slides/SGD.html#why",
    "title": "Stochastic Gradient Descent",
    "section": "3.6 Why?",
    "text": "3.6 Why?\n\nFix \\(\\theta\\in \\mathbb R^n.\\)\nWe want find some \\(h\\) s.t. \\(f(\\theta+h) < f(\\theta).\\)\n\\(f(\\theta+h) \\approx f(\\theta) + \\begin{bmatrix} & \\nabla f(\\theta) & \\\\\\end{bmatrix} \\cdot \\begin{bmatrix} \\\\ h \\\\ \\\\\\end{bmatrix}\\) for \\(h\\in \\mathbb R^n\\) with \\(\\left\\lVert h \\right\\rVert\\) small enough. So \\(f(\\theta+h)<f(\\theta) \\iff \\nabla f(\\theta) \\cdot h < 0.\\)"
  },
  {
    "objectID": "slides/SGD.html#why-gd-works",
    "href": "slides/SGD.html#why-gd-works",
    "title": "Stochastic Gradient Descent",
    "section": "Why GD works?",
    "text": "Why GD works?\n\nLet \\(f:\\mathbb R^n\\longrightarrow \\mathbb R\\) be any differentiable function.\nFix \\(\\theta\\in \\mathbb \\Theta.\\)\nWe assume that \\(\\nabla f(\\theta) \\neq \\mathbf{0}.\\)\nOur goal is to find some \\(h\\in \\mathbb R^n\\) such that \\(f(\\theta+h) < f(\\theta).\\)\nBy Taylor’s theorem, \\[\n\\begin{aligned}\n  f(\\theta+h) \\approx f(\\theta) +  \\begin{bmatrix}  & \\nabla f(\\theta) &  \\\\\\end{bmatrix} \\cdot \\begin{bmatrix}  \\\\ h \\\\  \\\\\\end{bmatrix}\n\\end{aligned}\n\\] for \\(h\\in \\mathbb R^n\\) with \\(\\left\\lVert h \\right\\rVert\\) small enough.\nSo for \\(\\left\\lVert h \\right\\rVert\\) small enough, \\(f(\\theta+h)<f(\\theta) \\iff \\nabla f(\\theta) \\cdot h < 0.\\)\nGiven \\(\\eta_0\\in (0,\\infty)\\) small. By Cauchy’s inequality, \\[\n\\begin{aligned}\n  \\operatorname{argmin}_{h\\in \\mathbb R^n,\\left\\lVert h \\right\\rVert = \\eta_0}   \n  \\nabla f(\\theta) \\cdot h\n  = -\\eta_0 \\cdot \\frac{\\nabla f(\\theta)}{\\left\\lVert \\nabla f(\\theta) \\right\\rVert}.\n\\end{aligned}\n\\]\nHence, we choose \\[\n\\begin{aligned}\n  h = -\\eta \\cdot \\nabla f(\\theta)\n\\end{aligned}\n\\] for some \\(\\eta\\in (0,\\infty)\\) small."
  },
  {
    "objectID": "slides/SGD.html#sec-Regression1",
    "href": "slides/SGD.html#sec-Regression1",
    "title": "Stochastic Gradient Descent",
    "section": "3.1 Regression",
    "text": "3.1 Regression\n\nWe see the case of \\(\\mathbb R.\\)\nSuppose that we have the data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R\\times \\mathbb R.\\)\n\nWe want to find some continuous function \\(\\color{red}{f}:\\mathbb R\\longrightarrow \\mathbb R\\) such that \\[\n\\begin{aligned}\n  f(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn math, our goal is to find \\[\n\\begin{aligned}\n  \\operatorname{argmin}_{w,b\\in \\mathbb R}\\sum_{i=1}^{n} \\ell \\bigl( y^{(i)} , f(x^{(i)}) \\bigr),\n\\end{aligned}\n\\] where \\(\\color{red}{\\ell}:\\mathbb R^2 \\longrightarrow [0,\\infty)\\) is what we need to determine.\nWhat is the best choice for \\(d\\) (in some sense)?\n\nWe may use the MLE (maximum likelihood function)."
  },
  {
    "objectID": "slides/SGD.html#sec-SGDsingle",
    "href": "slides/SGD.html#sec-SGDsingle",
    "title": "Stochastic Gradient Descent",
    "section": "Stochastic Gradient Descent(single)",
    "text": "Stochastic Gradient Descent(single)\n\nOur goal is to find a local min. of \\[\n\\begin{aligned}\n  L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n\\ell(y^{(i)},\\widehat{y}^{(i)}).\n\\end{aligned}\n\\]\nLet \\(\\ell_i(\\theta) = \\ell(y^{(i)},\\widehat{y}^{(i)}).\\)\nGiven any initial value \\(\\theta\\in \\mathbb \\Theta\\) and any \\(\\eta\\in (0,\\infty)\\) small enough.\nLet \\(P\\) be a permutation of \\(\\lbrace 1,2,\\cdots, n \\rbrace.\\)\nThen for each \\(j=1,\\cdots, n,\\) we update \\(\\theta\\) by \\[\n\\begin{aligned}\n\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla \\ell_{P(j)}(\\theta).\n\\end{aligned}\n\\]\nAfter repeating the above step several times, the final \\(\\theta\\) may be close to a local min. of \\(L.\\)\n\n\n\nThe SGD(single) can be an effective strategy, even for large datasets (Bottou 2010).\n\n\n\n可想像圖會在那裡抖"
  },
  {
    "objectID": "slides/SGD.html#why-sgdsingle-works",
    "href": "slides/SGD.html#why-sgdsingle-works",
    "title": "Stochastic Gradient Descent",
    "section": "Why SGD(single) works",
    "text": "Why SGD(single) works\n\nWe see the case of linear regression from \\(\\mathbb R\\) to \\(\\mathbb R.\\)\nFor simplicity, we assume that \\(b\\) is a fixed constant, not a parameter.\nNow our parameters are only \\(w.\\)\nWe rewrite \\[\n\\begin{aligned}\n  L(w)=\\sum_{i=1}^n \\bigl( y^{(i)} - (w x^{(i)} + b) \\bigr)^2\n\\end{aligned}\n\\] to \\[\n\\begin{aligned}\n  L(w) = \\sum_{i=1}^n \\bigl( a_i w - b_i  \\bigr)^2\n  \\stackrel{\\text{denote}}{=}\n  \\sum_{i=1}^n \\ell_i(w).\n\\end{aligned}\n\\]\nLet \\(w_i = b_i/a_i\\) and \\[\n\\begin{aligned}\n  w_* = \\min_i w_i , \\quad w^* = \\max_i w_i.\n\\end{aligned}\n\\]\nNote that \\[\n\\begin{aligned}\n  w_*  \n  \\leq \\operatorname{argmin}_{w\\in \\mathbb R} L(w)\n  = \\frac{\\sum_{i}a_i b_i}{\\sum_i a_i^2}\n  \\leq w^*.\n\\end{aligned}\n\\]\n\n\n\nFor each \\(i,\\) if we update \\(w\\) by \\[\n\\begin{aligned}\n  w' = w - \\eta \\cdot \\nabla_w  \\ell_i (w),\n\\end{aligned}\n\\] then \\(w'\\) will try to enter \\([w_*,w^*]\\) if \\(w'\\) not in this interval.\n\n\n\n補圖\n補救方式就是打包成 minibatch"
  },
  {
    "objectID": "slides/SGD.html#why-sgdminibatch-works",
    "href": "slides/SGD.html#why-sgdminibatch-works",
    "title": "Stochastic Gradient Descent",
    "section": "Why SGD(minibatch) works",
    "text": "Why SGD(minibatch) works\n\nFix \\(\\theta\\in \\Theta.\\)\nThe main idea here is to pick some random vector \\(X\\) such that \\(\\mathbf E[X]=\\nabla_{\\theta} L(\\theta).\\)\nRecall that our goal is to find a local min. of \\[\n\\begin{aligned}\n  L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n\\ell(y^{(i)},\\widehat{y}^{(i)}).\n\\end{aligned}\n\\]\nIf \\((X,Y)\\) is a random sample from \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\), then \\[\n\\begin{aligned}\n  \\mathbf{E} \\bigl[ \\nabla_\\theta \\ell\\bigl(Y,\\mathtt{Net}_\\theta(X)\\bigr)  \\bigr]\n  = \\frac{1}{n}  \\sum_{i=1}^n \\nabla_{\\theta} \\ell\\bigl( y^{(i)} , \\mathtt{Net}_{\\theta} (x^{(i)}) \\bigr)\n  = \\nabla_{\\theta} L(\\theta).\n\\end{aligned}\n\\]\nLet \\(\\bigl\\lbrace (X_k,Y_k) \\bigr\\rbrace_{k=1}^m\\) be a simple random sampling with replacement from \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)}).\\) Then \\[\n\\begin{aligned}\n  \\dfrac{1}{m}\\sum_{k=1}^m \\nabla_\\theta \\ell\\bigl(Y_k,\\mathtt{Net}_\\theta(X_k)\\bigr)\n\\end{aligned}\n\\] is an unbiased estimation of \\(\\nabla_{\\theta} L(\\theta).\\)\nMoreover, by SLLN, as \\(m\\rightarrow\\infty,\\) \\[\n\\begin{aligned}\n  \\dfrac{1}{m}\\sum_{k=1}^m \\nabla_\\theta \\ell\\bigl(Y_k,\\mathtt{Net}_\\theta(X_k)\\bigr) \\stackrel{\\text{a.s.}}{\\longrightarrow} \\nabla_{\\theta}L(\\theta).\n\\end{aligned}\n\\]\nIn practive, we use the simple random sampling without replacement for efficiency."
  },
  {
    "objectID": "slides/SGD.html#sec-Regression-Linear",
    "href": "slides/SGD.html#sec-Regression-Linear",
    "title": "Stochastic Gradient Descent",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nFor simplicity, we see linear regression from \\(\\mathbb R\\) to \\(\\mathbb R.\\)\nSuppose that we have the data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R\\times \\mathbb R.\\)\n\nWe want to find some linear function \\(f:\\mathbb R\\longrightarrow \\mathbb R,\\) say \\(f(x) = wx+b,\\) such that \\[\n\\begin{aligned}\n  f(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn math, our goal is to find \\[\n\\begin{aligned}\n  \\operatorname{argmin}_{w,b\\in \\mathbb R}\\sum_{i=1}^{n} \\ell \\bigl( y^{(i)} , f(x^{(i)}) \\bigr),\n\\end{aligned}\n\\] where \\(\\ell:\\mathbb R^2 \\longrightarrow [0,\\infty)\\) is what we need to determine.\nWhat is the best choice for \\(\\ell\\) (in some sense)?\n\nWe may use the MLE (maximum likelihood function)."
  },
  {
    "objectID": "slides/SGD.html#sec-Regression2",
    "href": "slides/SGD.html#sec-Regression2",
    "title": "Stochastic Gradient Descent",
    "section": "Regression",
    "text": "Regression\n\nSuppose that we have the data \\((x^{(1)},y^{(1)}), \\cdots , (x^{(n)},y^{(n)})\\) in \\(\\mathbb R^{\\text{in}}\\times \\mathbb R^{\\text{out}}.\\)\n\nWe first need to give a function set \\(\\lbrace f_\\theta: \\theta \\in \\Theta \\rbrace,\\) where each \\(f_\\theta\\) is a function from \\(\\mathbb R^{\\text{in}}\\) to \\(\\mathbb R^{\\text{out}}.\\)\nWe want to find some \\(\\theta\\in \\Theta\\) such that \\[\n\\begin{aligned}\n  f_\\theta(x^{(i)}) \\approx y^{(i)} , \\qquad \\forall  i = 1\\sim n.\n\\end{aligned}\n\\]\nIn math, our goal is to find \\[\n\\begin{aligned}\n  \\operatorname{argmin}_{\\theta\\in \\Theta} \\sum_{i=1}^{n} \\ell \\bigl( y^{(i)} , f_\\theta(x^{(i)}) \\bigr),\n\\end{aligned}\n\\] where \\(\\ell(y,\\widehat{y}) = (y-\\widehat{y})^2.\\)"
  },
  {
    "objectID": "slides/SGD.html#comparison-of-gd-and-sgd",
    "href": "slides/SGD.html#comparison-of-gd-and-sgd",
    "title": "Stochastic Gradient Descent",
    "section": "Comparison of GD and SGD",
    "text": "Comparison of GD and SGD"
  },
  {
    "objectID": "slides/SGD.html#what-next",
    "href": "slides/SGD.html#what-next",
    "title": "Stochastic Gradient Descent",
    "section": "What next?",
    "text": "What next?\n\nHow to calculate \\(\\nabla_{\\theta} L(\\theta)\\)?\nThe above is dependent on \\(\\mathtt{Net}_{\\theta},\\) so what kind of \\(\\mathtt{Net}_{\\theta}\\) will we use?"
  },
  {
    "objectID": "slides/SGD.html#reference",
    "href": "slides/SGD.html#reference",
    "title": "Stochastic Gradient Descent",
    "section": "Reference",
    "text": "Reference\n\n\n\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\nBottou, Léon. 2010. “Large-Scale Machine Learning with Stochastic Gradient Descent.” In Proceedings of COMPSTAT’2010, 177–86. Springer.\n\n\nLiu, Dong C, and Jorge Nocedal. 1989. “On the Limited Memory BFGS Method for Large Scale Optimization.” Mathematical Programming 45 (1): 503–28."
  },
  {
    "objectID": "slides/SGD.html#whats-next",
    "href": "slides/SGD.html#whats-next",
    "title": "Stochastic Gradient Descent",
    "section": "What’s next?",
    "text": "What’s next?\n\nWhat kind of \\(\\mathtt{Net}_{\\theta}\\) can we choose?\nHow to calculate \\(\\nabla_{\\theta} L(\\theta)\\)?\nThe above is dependent on \\(\\mathtt{Net}_{\\theta},\\) so what kind of \\(\\mathtt{Net}_{\\theta}\\) will we use?\nWhat is the best choice for \\(\\ell\\) for the classification?"
  },
  {
    "objectID": "temp/test.html",
    "href": "temp/test.html",
    "title": "Test",
    "section": "",
    "text": "if A\n\nthen B\nC\n\nend if\n\nINPUT: input;\nOUTPUT: result;\n\nIF this_is_True:\n  do_this;\nELSE\n  select B from input;\n  do something whith input;\n  FOR EACH \\(a_i\\) in B\n    do something with \\(a_i\\);\n\n\n\n\n\n\nAlog\n\n\n\nInputs Given a Network \\(G=(V,E)\\) with flow capacity \\(c\\), a source node \\(s\\), and a sink node \\(t\\)\nOutput Compute a flow \\(f\\) from \\(s\\) to \\(t\\) of maximum value\n\n\\(f(u, v) \\leftarrow 0\\) for all edges \\((u,v)\\)\nWhile there is a path \\(p\\) from \\(s\\) to \\(t\\) in \\(G_{f}\\) such that \\(c_{f}(u,v)>0\\) for all edges \\((u,v) \\in p\\):\n\nFind \\(c_{f}(p)= \\min \\{c_{f}(u,v):(u,v)\\in p\\}\\)\nFor each edge \\((u,v) \\in p\\)\n\n\\(f(u,v) \\leftarrow f(u,v) + c_{f}(p)\\) (Send flow along the path)\n\\(f(u,v) \\leftarrow f(u,v) - c_{f}(p)\\) (The flow might be “returned” later)\n\n\n\n\n\n\n\nst=>start: Start\nop=>operation: Your Operation\ncond=>condition: Yes or No?\ne=>end\n\nst->op->cond\ncond(yes)->e\ncond(no)->op\nAlice->Bob: Hello Bob, how are you?\nNote right of Bob: Bob thinks\nBob-->Alice: I am good thanks!\npie\n    title Pie Chart\n    \"Dogs\" : 386\n    \"Cats\" : 85\n    \"Rats\" : 150"
  },
  {
    "objectID": "temp/test.html#alog",
    "href": "temp/test.html#alog",
    "title": "Test",
    "section": "Alog",
    "text": "Alog\n\nInputs Given a Network \\(G=(V,E)\\) with flow capacity \\(c\\), a source node \\(s\\), and a sink node \\(t\\)\nOutput Compute a flow \\(f\\) from \\(s\\) to \\(t\\) of maximum value\n\n\\(f(u, v) \\leftarrow 0\\) for all edges \\((u,v)\\)\nWhile there is a path \\(p\\) from \\(s\\) to \\(t\\) in \\(G_{f}\\) such that \\(c_{f}(u,v)>0\\) for all edges \\((u,v) \\in p\\):\n\nFind \\(c_{f}(p)= \\min \\{c_{f}(u,v):(u,v)\\in p\\}\\)\nFor each edge \\((u,v) \\in p\\)\n\n\\(f(u,v) \\leftarrow f(u,v) + c_{f}(p)\\) (Send flow along the path)\n\\(f(u,v) \\leftarrow f(u,v) - c_{f}(p)\\) (The flow might be “returned” later)\n\n\n\n:::\n\n\nst=>start: Start\nop=>operation: Your Operation\ncond=>condition: Yes or No?\ne=>end\n\nst->op->cond\ncond(yes)->e\ncond(no)->op\nAlice->Bob: Hello Bob, how are you?\nNote right of Bob: Bob thinks\nBob-->Alice: I am good thanks!\npie\n    title Pie Chart\n    \"Dogs\" : 386\n    \"Cats\" : 85\n    \"Rats\" : 150"
  },
  {
    "objectID": "TODO.html#markdown",
    "href": "TODO.html#markdown",
    "title": "TODO",
    "section": "markdown",
    "text": "markdown\n\nhttps://support.typora.io/Draw-Diagrams-With-Markdown/\nhttps://saswat.padhi.me/pseudocode.js/"
  },
  {
    "objectID": "slides/SGD.html#asd",
    "href": "slides/SGD.html#asd",
    "title": "Stochastic Gradient Descent",
    "section": "ASD",
    "text": "ASD"
  },
  {
    "objectID": "slides/SGD.html#gradient-descentgd",
    "href": "slides/SGD.html#gradient-descentgd",
    "title": "Stochastic Gradient Descent",
    "section": "Gradient Descent(GD)",
    "text": "Gradient Descent(GD)\n\n\n\nOur goal is to find a local min. of \\[\n\\begin{aligned}\n  L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n\\ell(y^{(i)},\\widehat{y}^{(i)}).\n\\end{aligned}\n\\]\nGiven any initial value \\(\\theta_0\\in \\mathbb \\Theta.\\)\nGiven any \\(\\eta\\in (0,\\infty)\\) small enough.\n\n\\(\\eta\\): learning rate.\n\nMomentum, Adam, \\(\\cdots.\\)\n\n\nWe set \\(\\lbrace \\theta_i \\rbrace_{i\\in \\mathbb N}\\) by \\[\n\\begin{aligned}\n\\theta_{i} \\leftarrow \\theta_{i-1} - \\eta \\cdot \\nabla L(\\theta_{i-1}),\\quad i\\in \\mathbb N.\n\\end{aligned}\n\\]\nThen \\(\\theta_i,i\\in \\mathbb N\\) may converges to a local min. of \\(L.\\)\n\n\n\n\n\n\n\n\n\nIn practice, this can be extremely slow: we must pass over the entire dataset before making a single update, even if the update steps might be very powerful (Liu and Nocedal 1989).\n\nSolution: Stochastic Gradient Descent (SGD).\n\n\n\n\n取 \\(1/n\\) 是之後有個方便地方."
  }
]